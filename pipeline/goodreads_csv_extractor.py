#!/usr/bin/env python3
"""
GOODREADS CSV EXTRACTOR
====================================
Architecture: Nested Dataclasses
--------------------------------
This script reads a Goodreads export CSV and structures it into a 
composite 'Book' object that separates data by source.

Data Structure:
  Book
  ‚îú‚îÄ‚îÄ goodreads (GoodreadsData) -> Populated by this script
  ‚îú‚îÄ‚îÄ google (GoogleBooksData)  -> Placeholder for future API script
  ‚îî‚îÄ‚îÄ llm (LLMData)             -> Placeholder for future AI script

HOW TO ADD A NEW FIELD:
--------------------------
1. Add the new field to the appropriate dataclass (e.g., `cover_image: str = ""`).
2. Add the field to 'FIELD_MAPPING' if it exists in the CSV (e.g., 'Cover URL': 'cover_image').
3. Update 'row_to_book' to extract the data (e.g., `cover_image=get_val('Cover URL')`).
4. (Optional) If it requires complex cleaning, write a helper function (like 'clean_isbn').

"""

import pandas as pd
import json
import os
import logging
from dataclasses import dataclass, asdict, field, fields
from typing import Dict, List, Any, Optional
import glob
import sys
from datetime import datetime

# Add the project root to the Python path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from config import GOODREADS_CSV_PATTERN, LIBRARY_PATH, FILTER_READ_ONLY, LOG_DIR

#### CONFIGURATION ####
logger = logging.getLogger("Goodreads_Ingestion")

def setup_logger(timestamp: str):
    """Configures a logger that syncs with the experiment_runner timestamp."""
    log_dir = LOG_DIR
    os.makedirs(log_dir, exist_ok=True)
    
    logger = logging.getLogger("Goodreads_Ingestion")
    logger.setLevel(logging.DEBUG) # Capture everything
    
    # Prevent duplicate logs if run multiple times in same session
    if not logger.handlers: 
        fh = logging.FileHandler(os.path.join(log_dir, f"goodreads_ingest_{timestamp}.log"), encoding='utf-8')
        fh.setLevel(logging.DEBUG)
        
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO) # Keep console clean, only show INFO
        
        formatter = logging.Formatter('%(asctime)s | [%(levelname)s] | %(message)s', datefmt='%H:%M:%S')
        fh.setFormatter(formatter)
        ch.setFormatter(formatter)
        
        logger.addHandler(fh)
        logger.addHandler(ch)
        
#### DATA CLASSES ####

@dataclass
class GoodreadsData:
    """
    Data strictly from the Goodreads CSV export. 
    All Goodreads columns are included.
    """
    id: str
    title: str
    author: str
    author_lf: str
    additional_authors: str
    isbn: str = ""
    isbn13: str = ""
    my_rating: int = 0
    average_rating: float = 0.0
    publisher: str = ""
    binding: str = ""
    pages: int = 0
    pub_year: Optional[int] = None
    original_pub_year: Optional[int] = None
    date_read: str = ""
    date_added: str = ""
    bookshelves: str = ""
    bookshelves_w_positions: str = ""
    exclusive_shelf: str = ""
    my_review: str = ""
    spoiler: str = ""
    private_notes: str = ""
    read_count: int = 0  
    owned_copies: int = 0
    description: str = ""

@dataclass
class GoogleBooksData:
    """Data strictly from the Google Books API (Future Use)."""
    id: str = ""
    title: str = ""
    subtitle: str = ""
    authors: List[str] = field(default_factory=list)
    description: str = ""
    categories: List[str] = field(default_factory=list)
    published_date: str = ""
    thumbnail_url: str = ""
    page_count: int = 0
    language: str = ""

    # This variable holds everything else GoogleBooks returned
    raw_data: Dict[str, Any] = field(default_factory=dict)

@dataclass
class LLMData:
    """Data generated by LLM analysis."""
    category: str = ""
    genre: str = ""
    sphere: str = ""
    era: str = ""
    location: List[str] = field(default_factory=list)
    validated: bool = False 
    ai_snapshot: Dict[str, Any] = field(default_factory=dict)

#### MAIN BOOK DATA CLASS ---

@dataclass
class Book:
    """
    The master container. 
    It holds the sub-components.
    """
    goodreads: GoodreadsData
    # These fields automatically start as empty objects if not provided
    google: GoogleBooksData = field(default_factory=GoogleBooksData)
    llm: LLMData = field(default_factory=LLMData)

    def to_dict(self):
        """Converts the entire nested structure to a dictionary for JSON saving."""
        return asdict(self)

    @classmethod
    def from_dict(cls, data: Dict):
        """
        Reconstructs a Book object from a dictionary.
        Safely ignores legacy keys that no longer exist in the dataclasses.
        """
        gr_data = data.get('goodreads', {})
        google_data = data.get('google', {})
        llm_data = data.get('llm', {})

        # Dynamically find the valid keys for each dataclass
        valid_gr = {f.name for f in fields(GoodreadsData)}
        valid_google = {f.name for f in fields(GoogleBooksData)}
        valid_llm = {f.name for f in fields(LLMData)}

        # Filter the dictionaries to ONLY include keys that currently exist
        clean_gr = {k: v for k, v in gr_data.items() if k in valid_gr}
        clean_google = {k: v for k, v in google_data.items() if k in valid_google}
        clean_llm = {k: v for k, v in llm_data.items() if k in valid_llm}

        return cls(
            goodreads=GoodreadsData(**clean_gr),
            google=GoogleBooksData(**clean_google),
            llm=LLMData(**clean_llm)
        )


#### HELPER FUNCTIONS ####

def clean_isbn(val: Any) -> str:
    """Removes Excel formatting from ISBNs."""
    if pd.isna(val) or not val: 
        return ''
    s = str(val).replace('=', '').replace('"', '').strip()
    return ''.join(c for c in s if c.isdigit())

def parse_year(val: Any) -> Optional[int]:
    """Safely parses a year."""
    try: 
        return int(float(val)) if pd.notna(val) else None
    except (ValueError, TypeError): 
        return None

def process_tags(val: Any) -> List[str]:
    """Splits comma-separated tags."""
    if not val or pd.isna(val): 
        return []
    tags = [t.strip() for t in str(val).split(',')]
    return [t for t in tags if t and t.lower() != 'read']

def get_str(row: pd.Series, col: str) -> str:
    """Safely extracts a string from a row, handling NaN/Empty."""
    val = row.get(col)
    return str(val).strip() if pd.notna(val) else ""

def get_int(row: pd.Series, col: str) -> int:
    """Safely extracts an int, defaults to 0 on error."""
    val = row.get(col)
    try:
        return int(float(val)) if pd.notna(val) else 0
    except (ValueError, TypeError):
        return 0

def get_float(row: pd.Series, col: str) -> float:
    """Safely extracts a float, defaults to 0.0 on error."""
    val = row.get(col)
    try:
        return float(val) if pd.notna(val) else 0.0
    except (ValueError, TypeError):
        return 0.0

#### CORE PROCESS FUNCTIONS ####

def load_and_filter_csv(csv_path: str, filter_read_only: bool = True, limit: int = 0) -> pd.DataFrame:
    """Loads CSV and applies the 'Exclusive Shelf' filter if requested."""
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"CSV not found: {csv_path}")
    
    logger.info(f"Reading CSV from: {csv_path}")
    df = pd.read_csv(csv_path)
    
    # --- FILTER LOGIC ---
    if filter_read_only:
        if 'Exclusive Shelf' in df.columns:
            original = len(df)
            df = df[df['Exclusive Shelf'] == 'read']
            logger.info(f"Filtered to {len(df)} 'read' books (dropped {original - len(df)}).")
        else:
            logger.warning("'Exclusive Shelf' column missing. Cannot filter by read status.")
    
    if limit > 0:
        df = df.head(limit)
        logger.info(f"Limiting processing to first {limit} rows.")
        
    return df

def row_to_book(row: pd.Series) -> Book:
    """
    Converts a CSV row into a Book object.
    Maps CSV columns to the GoodreadsData dataclass.
    """
    
    # Create the Goodreads Component
    gr_component = GoodreadsData(
        id=get_str(row, 'Book Id'),
        title=get_str(row, 'Title'),
        author=get_str(row, 'Author'),
        author_lf=get_str(row, 'Author l-f'),
        additional_authors=get_str(row, 'Additional Authors'),
        isbn=clean_isbn(row.get('ISBN')),
        isbn13=clean_isbn(row.get('ISBN13')),
        my_rating=get_int(row, 'My Rating'),
        average_rating=get_float(row, 'Average Rating'),
        publisher=get_str(row, 'Publisher'),
        binding=get_str(row, 'Binding'),
        pages=get_int(row, 'Number of Pages'),
        pub_year=parse_year(row.get('Year Published')),
        original_pub_year=parse_year(row.get('Original Publication Year')),
        date_read=get_str(row, 'Date Read'),
        date_added=get_str(row, 'Date Added'),
        bookshelves=get_str(row, 'Bookshelves'),
        bookshelves_w_positions=get_str(row, 'Bookshelves with positions'),
        exclusive_shelf=get_str(row, 'Exclusive Shelf'),
        my_review=get_str(row, 'My Review'),
        spoiler=get_str(row, 'Spoiler'),
        private_notes=get_str(row, 'Private Notes'),
        read_count=get_int(row, 'Read Count'),
        owned_copies=get_int(row, 'Owned Copies'),
        description=""
    )

    # Wrap it in the main Book container
    # The 'google' and 'llm' slots will be created as empty defaults automatically
    return Book(goodreads=gr_component)

def merge_books(new_books: List[Book], existing_books: List[Book]) -> List[Book]:
    """
    Merges new CSV data into an existing list of Book objects in-memory.
    Updates 'goodreads' data but PROTECTS 'google' and 'llm' data.
    """
    # Convert list -> Dictionary for fast lookup (Key = ID)
    existing_map = {b.goodreads.id: b for b in existing_books}
    merged_list = []

    for new_book in new_books:
        book_id = new_book.goodreads.id
        
        if book_id in existing_map:
            # === MERGE STRATEGY ===
            existing_book = existing_map[book_id]
            # 1. Update Goodreads Data
            # If the existing book has a description (because this comes from the scaper, not CSV) but the new CSV data doesn't, keep the old one.
            if existing_book.goodreads.description and not new_book.goodreads.description:
                new_book.goodreads.description = existing_book.goodreads.description
            existing_book.goodreads = new_book.goodreads
            # 2. Keep Google/LLM Data (implicitly preserved)
            merged_list.append(existing_book)
        else:
            merged_list.append(new_book)
            
    return merged_list

def main():
    """CLI WRAPPER: Handles disk I/O and runs the process."""
    try:

        # Initialize Logging
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        setup_logger(timestamp)
        
        logger.info("--- Starting Export Process ---")
        
        # Find the freshest CSV using Glob
        matching_files = glob.glob(GOODREADS_CSV_PATTERN)
        if not matching_files:
            logger.error(f"‚ùå No CSV files found matching: {GOODREADS_CSV_PATTERN}")
            return
        
        # Automatically pick the newest file if there are multiple exports
        latest_csv_path = max(matching_files, key=os.path.getmtime)
        logger.info(f"üìÑ Found dataset: {latest_csv_path}")
        
        # Load Data from CSV
        df = load_and_filter_csv(latest_csv_path, filter_read_only=FILTER_READ_ONLY, limit=0)
        
        # Transform CSV to Book Objects
        new_books_list = [row_to_book(row) for _, row in df.iterrows()]
        logger.info(f"Transformation complete. Processed {len(new_books_list)} books.")
        
        # Load Existing Library from Disk (I/O)
        existing_books = []
        if os.path.exists(LIBRARY_PATH):
            with open(LIBRARY_PATH, 'r', encoding='utf-8') as f:
                raw_list = json.load(f)
                existing_books = [Book.from_dict(item) for item in raw_list]
        
        # Create a fast lookup set of the IDs we already have
        existing_ids = {b.goodreads.id for b in existing_books}
        # Count how many of the incoming books have an ID not currently in our library
        new_added_count = sum(1 for b in new_books_list if b.goodreads.id not in existing_ids)
        total_read_count = len(new_books_list)

        # Merge Data in memory (Core Logic)
        final_books = merge_books(new_books_list, existing_books)
        
        # Save back to Disk (I/O)
        final_data = [b.to_dict() for b in final_books]
        final_data.sort(key=lambda x: x['goodreads']['id'])
        
        with open(LIBRARY_PATH, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, indent=4, ensure_ascii=False)
            
        logger.info(f"Success! Saved {len(final_data)} books to {LIBRARY_PATH}")

        # Return the stats back to whatever called this function!
        return {
            "total_read": total_read_count,
            "new_added": new_added_count
        }

    except Exception as e:
        logger.error(f"Process Failed: {e}")
        raise

if __name__ == "__main__":
    main()